
# ğŸ§  Backblaze Hard Drive Failure Prediction

### End-to-End Big-Data ML System for Rare Event Prediction

---

## ğŸ” Project Overview

This project implements a **production-style failure prediction system** for hard drives using Backblaze SMART telemetry data.

The objective is not just to train a model, but to **design, evaluate, and stress-test an end-to-end ML system** under real-world constraints:

* Millions of rows of time-series data
* Extreme class imbalance (â‰ˆ 1 failure in 20,000 samples)
* Strict temporal integrity (no data leakage)
* Operationally meaningful evaluation beyond PR-AUC

The result is a **Spark-first, deployment-oriented pipeline** that demonstrates how predictive maintenance systems behave in practice â€” including their limitations.

---

## ğŸ¯ Problem Statement

Predict whether a hard drive will fail **within the next 24 hours**, using SMART attributes recorded daily.

This is a **rare-event prediction problem** with severe asymmetry in cost:

* Missing a failure â†’ potential data loss
* Excessive false alerts â†’ operational overload

---

## ğŸ§© Core Challenges Addressed

* **Extreme class imbalance** (â‰ˆ 0.005% positives)
* **Time-dependent labels** (future failures)
* **Scalability** to millions of rows
* **Mismatch between model metrics and operational impact**
* **Explainability under noisy, sparse features**

---

## ğŸ—ï¸ System Architecture

```
Raw SMART Logs (Parquet)
        â†“
Spark-based Feature Processing
        â†“
Time-aware Label Generation
        â†“
Train / Validation / Test Split (chronological)
        â†“
Downsampled Training (Spark â†’ Pandas)
        â†“
XGBoost Model Training
        â†“
Full-scale Spark Inference
        â†“
Operational Impact Simulation
        â†“
SHAP-based Alert Explanation
```

---

## ğŸ“ Repository Structure

```
.
â”œâ”€â”€ data/
â”‚   â”œâ”€â”€ raw/                 # Original Backblaze SMART logs
â”‚   â”œâ”€â”€ processed/           # Feature-engineered datasets
â”‚   â””â”€â”€ model_ready/         # Time-based train/val/test splits
â”‚
â”œâ”€â”€ models/
â”‚   â””â”€â”€ xgboost_backblaze.json
â”‚
â”œâ”€â”€ outputs/
â”‚   â”œâ”€â”€ daily_alerts.parquet
â”‚   â””â”€â”€ shap_alert_summary.png
â”‚
â”œâ”€â”€ scripts/
â”‚   â”œâ”€â”€ 01_data_processing.py
â”‚   â”œâ”€â”€ 02_label_generation.py
â”‚   â”œâ”€â”€ 03_train_val_test_split.py
â”‚   â”œâ”€â”€ 04_model_training.py
â”‚   â”œâ”€â”€ 05_SHAP_alert_explanation.py
â”‚   â””â”€â”€ 06_deployment_code.py
â”‚
â””â”€â”€ README.md
```

---

## ğŸ”„ Data Splitting Strategy (No Leakage)

To preserve temporal causality, the dataset is split **strictly by time**:

| Split      | Purpose                                |
| ---------- | -------------------------------------- |
| Train      | Model fitting                          |
| Validation | Hyperparameter tuning / early stopping |
| Test       | Future unseen data                     |

Random splits are **explicitly avoided**.

---

## âš–ï¸ Handling Extreme Imbalance

* Raw class ratio â‰ˆ **1 : 25,000**
* Strategy:

  * Negative class downsampling **only in training**
  * `scale_pos_weight` used in XGBoost
* Validation and test sets remain **fully imbalanced** to reflect reality

This ensures:

* Learnable signal during training
* Honest evaluation during inference

---

## ğŸ¤– Model Choice

**XGBoost (Gradient Boosted Trees)**

Why:

* Strong performance on tabular data
* Handles non-linear feature interactions
* Robust to missing values
* Widely used in industrial reliability systems

**Evaluation metric:** PR-AUC
(ROC-AUC is misleading under extreme imbalance)

---

## ğŸš€ Deployment-Style Inference

* Full test set scored using **Spark**
* XGBoost inference applied via Spark UDF
* Produces per-drive, per-day failure probabilities

This simulates **real batch deployment**, not offline evaluation.

---

## ğŸ“Š Operational Impact Analysis

Instead of stopping at PR-AUC, the system is evaluated in operational terms:

* Detected failures (TP)
* Missed failures (FN)
* False alarms (FP)
* Alerts per day

### Example Outcome

```
Total test failures:        248
Detected failures (TP):     202
Missed failures (FN):       46
False alarms (FP):          2,525,188

Recall:                     0.81
Precision:                  0.0001
Avg alerts per day:         157,836
```

ğŸ” **Key Insight**
A model can achieve high recall yet be **operationally unusable** due to alert fatigue â€” a common failure mode in rare-event ML systems.

---

## ğŸ” Model Interpretability (SHAP)

SHAP analysis is applied to alerted samples to understand model behavior.

Findings:

* Model correctly emphasizes SMART attributes linked to disk degradation
* Sparse counters and noisy signals cause large SHAP variance
* Explains alert explosion despite reasonable recall

This reinforces the need for **policy-based alerting**, not naive probability thresholds.

---

## ğŸ§  Key Takeaways

* Rare-event prediction is a **systems problem**, not just a modeling problem
* PR-AUC alone is insufficient for deployment decisions
* Threshold-based alerting breaks down under extreme imbalance
* Spark-first pipelines are mandatory at this scale
* Explainability is critical for diagnosing failure modes

---

## ğŸš§ Known Limitations (Intentional)

* No probability calibration
* No ranking-based alert budget (Top-K/day)
* No online learning loop

These are consciously excluded to keep the project focused on **core system design principles**.

---

## ğŸ Project Status

**Complete.**
This repository is intended as a **capability demonstration** for:

* Big-data ML engineering
* Failure prediction systems
* Imbalanced learning
* Deployment-oriented evaluation

---

## ğŸ‘¤ Author

**Nayan**
Interests:

* Large-scale ML systems
* Reliability & predictive maintenance
* AI for operational decision-making


