{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5640aebc",
   "metadata": {},
   "source": [
    "## Data Ingestion & Optimization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5bb93e52",
   "metadata": {},
   "source": [
    "**Objective:** Raw CSVs are slow and heavy. We need to load them using PySpark (because Pandas will crash with Memory Errors) and convert them to Parquet."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "04221472",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col, to_date, count, when, isnan, stddev"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2d06d352",
   "metadata": {},
   "outputs": [
    {
     "ename": "PySparkRuntimeError",
     "evalue": "[JAVA_GATEWAY_EXITED] Java gateway process exited before sending its port number.",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mPySparkRuntimeError\u001b[39m                       Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[3]\u001b[39m\u001b[32m, line 7\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;66;03m# --- INDUSTRY TECHNIQUE: CONFIGURING SPARK ---\u001b[39;00m\n\u001b[32m      2\u001b[39m \u001b[38;5;66;03m# We configure the driver memory to ensure we don't OOM (Out of Memory) locally.\u001b[39;00m\n\u001b[32m      3\u001b[39m \u001b[38;5;66;03m# In a cluster (Databricks/AWS EMR), this is handled by the cluster manager.\u001b[39;00m\n\u001b[32m      4\u001b[39m spark = \u001b[43mSparkSession\u001b[49m\u001b[43m.\u001b[49m\u001b[43mbuilder\u001b[49m\u001b[43m \u001b[49m\u001b[43m\\\u001b[49m\n\u001b[32m      5\u001b[39m \u001b[43m    \u001b[49m\u001b[43m.\u001b[49m\u001b[43mappName\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mBackblaze_Failure_Prediction\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[43m\\\u001b[49m\n\u001b[32m      6\u001b[39m \u001b[43m    \u001b[49m\u001b[43m.\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mspark.driver.memory\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43m8g\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[43m\\\u001b[49m\n\u001b[32m----> \u001b[39m\u001b[32m7\u001b[39m \u001b[43m    \u001b[49m\u001b[43m.\u001b[49m\u001b[43mgetOrCreate\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m      9\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mSpark Session Created\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Nayan\\Projects\\Big-Data-Backblaze-Hard-Drive-Failure\\.venv\\Lib\\site-packages\\pyspark\\sql\\session.py:556\u001b[39m, in \u001b[36mSparkSession.Builder.getOrCreate\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    554\u001b[39m     sparkConf.set(key, value)\n\u001b[32m    555\u001b[39m \u001b[38;5;66;03m# This SparkContext may be an existing one.\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m556\u001b[39m sc = \u001b[43mSparkContext\u001b[49m\u001b[43m.\u001b[49m\u001b[43mgetOrCreate\u001b[49m\u001b[43m(\u001b[49m\u001b[43msparkConf\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    557\u001b[39m \u001b[38;5;66;03m# Do not update `SparkConf` for existing `SparkContext`, as it's shared\u001b[39;00m\n\u001b[32m    558\u001b[39m \u001b[38;5;66;03m# by all sessions.\u001b[39;00m\n\u001b[32m    559\u001b[39m session = SparkSession(sc, options=\u001b[38;5;28mself\u001b[39m._options)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Nayan\\Projects\\Big-Data-Backblaze-Hard-Drive-Failure\\.venv\\Lib\\site-packages\\pyspark\\core\\context.py:523\u001b[39m, in \u001b[36mSparkContext.getOrCreate\u001b[39m\u001b[34m(cls, conf)\u001b[39m\n\u001b[32m    521\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m SparkContext._lock:\n\u001b[32m    522\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m SparkContext._active_spark_context \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m523\u001b[39m         \u001b[43mSparkContext\u001b[49m\u001b[43m(\u001b[49m\u001b[43mconf\u001b[49m\u001b[43m=\u001b[49m\u001b[43mconf\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mSparkConf\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    524\u001b[39m     \u001b[38;5;28;01massert\u001b[39;00m SparkContext._active_spark_context \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    525\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m SparkContext._active_spark_context\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Nayan\\Projects\\Big-Data-Backblaze-Hard-Drive-Failure\\.venv\\Lib\\site-packages\\pyspark\\core\\context.py:205\u001b[39m, in \u001b[36mSparkContext.__init__\u001b[39m\u001b[34m(self, master, appName, sparkHome, pyFiles, environment, batchSize, serializer, conf, gateway, jsc, profiler_cls, udf_profiler_cls, memory_profiler_cls)\u001b[39m\n\u001b[32m    199\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m gateway \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m gateway.gateway_parameters.auth_token \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    200\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[32m    201\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mYou are trying to pass an insecure Py4j gateway to Spark. This\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    202\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33m is not allowed as it is a security risk.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    203\u001b[39m     )\n\u001b[32m--> \u001b[39m\u001b[32m205\u001b[39m \u001b[43mSparkContext\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_ensure_initialized\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgateway\u001b[49m\u001b[43m=\u001b[49m\u001b[43mgateway\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconf\u001b[49m\u001b[43m=\u001b[49m\u001b[43mconf\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    206\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m    207\u001b[39m     \u001b[38;5;28mself\u001b[39m._do_init(\n\u001b[32m    208\u001b[39m         master,\n\u001b[32m    209\u001b[39m         appName,\n\u001b[32m   (...)\u001b[39m\u001b[32m    219\u001b[39m         memory_profiler_cls,\n\u001b[32m    220\u001b[39m     )\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Nayan\\Projects\\Big-Data-Backblaze-Hard-Drive-Failure\\.venv\\Lib\\site-packages\\pyspark\\core\\context.py:444\u001b[39m, in \u001b[36mSparkContext._ensure_initialized\u001b[39m\u001b[34m(cls, instance, gateway, conf)\u001b[39m\n\u001b[32m    442\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m SparkContext._lock:\n\u001b[32m    443\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m SparkContext._gateway:\n\u001b[32m--> \u001b[39m\u001b[32m444\u001b[39m         SparkContext._gateway = gateway \u001b[38;5;129;01mor\u001b[39;00m \u001b[43mlaunch_gateway\u001b[49m\u001b[43m(\u001b[49m\u001b[43mconf\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    445\u001b[39m         SparkContext._jvm = SparkContext._gateway.jvm\n\u001b[32m    447\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m instance:\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Nayan\\Projects\\Big-Data-Backblaze-Hard-Drive-Failure\\.venv\\Lib\\site-packages\\pyspark\\java_gateway.py:111\u001b[39m, in \u001b[36mlaunch_gateway\u001b[39m\u001b[34m(conf, popen_kwargs)\u001b[39m\n\u001b[32m    108\u001b[39m     time.sleep(\u001b[32m0.1\u001b[39m)\n\u001b[32m    110\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m os.path.isfile(conn_info_file):\n\u001b[32m--> \u001b[39m\u001b[32m111\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m PySparkRuntimeError(\n\u001b[32m    112\u001b[39m         errorClass=\u001b[33m\"\u001b[39m\u001b[33mJAVA_GATEWAY_EXITED\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m    113\u001b[39m         messageParameters={},\n\u001b[32m    114\u001b[39m     )\n\u001b[32m    116\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mopen\u001b[39m(conn_info_file, \u001b[33m\"\u001b[39m\u001b[33mrb\u001b[39m\u001b[33m\"\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m info:\n\u001b[32m    117\u001b[39m     gateway_port = read_int(info)\n",
      "\u001b[31mPySparkRuntimeError\u001b[39m: [JAVA_GATEWAY_EXITED] Java gateway process exited before sending its port number."
     ]
    }
   ],
   "source": [
    "# --- INDUSTRY TECHNIQUE: CONFIGURING SPARK ---\n",
    "# We configure the driver memory to ensure we don't OOM (Out of Memory) locally.\n",
    "# In a cluster (Databricks/AWS EMR), this is handled by the cluster manager.\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"Backblaze_Failure_Prediction\") \\\n",
    "    .config(\"spark.driver.memory\", \"8g\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "print(\"Spark Session Created\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9424e5e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- PATHS ---\n",
    "# Update this to where you unzipped the data\n",
    "RAW_DATA_PATH = \"./data/data_Q3_2025/*.csv\" \n",
    "OUTPUT_PATH = \"./data/parquet_Q3_2025\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "630917c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- STEP 1: READ RAW DATA ---\n",
    "# We use *.csv to tell Spark to read ALL files in that directory as one DataFrame.\n",
    "# inferSchema=True is convenient but slow. In production, we define schemas manually.\n",
    "print(\"Reading CSVs... this might take a minute.\")\n",
    "df = spark.read.csv(RAW_DATA_PATH, header=True, inferSchema=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d5f4f0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- STEP 2: INITIAL INSPECTION ---\n",
    "print(f\"Total Rows: {df.count()}\")\n",
    "print(\"Schema:\")\n",
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5954573",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's check the Class Imbalance immediately\n",
    "print(\"Distribution of Failures (0 = Healthy, 1 = Failed):\")\n",
    "df.groupBy(\"failure\").count().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1d27bca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- STEP 3: DATA CLEANING (LIGHT) ---\n",
    "# 1. Cast date column to actual date type\n",
    "df = df.withColumn(\"date\", to_date(col(\"date\"), \"yyyy-MM-dd\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17c83574",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter specifically for Seagate drives (Industry Standard: Don't mix manufacturers in one model initially)\n",
    "# Let's look at the most popular model in the dataset first to decide.\n",
    "print(\"Top 5 Hard Drive Models:\")\n",
    "df.groupBy(\"model\").count().orderBy(col(\"count\").desc()).show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f69474e",
   "metadata": {},
   "source": [
    "1. Filter for our Manufacturer (Seagate).\n",
    "2. Calculate the percentage of missing values for every column.\n",
    "3. Drop columns that are >90% Empty.\n",
    "4. Drop columns that have zero variance (the value is always \"0\" or always \"100\")."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79b5d3c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Filter for Seagate (ST) drives immediately\n",
    "# We do this because different manufacturers use different columns.\n",
    "# Analyzing \"Nulls\" across mixed manufacturers is misleading.\n",
    "\n",
    "# (Assuming 'ST4000DM000' or similar Seagate is top. Let's filter for just Seagate to reduce noise)\n",
    "# Note: You might need to adjust the model string based on the print output above.\n",
    "\n",
    "df_seagate = df.filter(col(\"model\").contains(\"ST\"))\n",
    "\n",
    "print(f\"Seagate Drives Count: {df_seagate.count()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5089c41",
   "metadata": {},
   "outputs": [],
   "source": [
    "#INDUSTRY TECHNIQUE: AUTOMATED NULL ANALYSIS\n",
    "# We want to find which columns are actually populated.\n",
    "print(\"Calculating Null percentages per column... (This takes time)\")\n",
    "\n",
    "# Get total row count for calculation\n",
    "total_rows = df_seagate.count()\n",
    "\n",
    "# Create a list to store valid columns\n",
    "valid_features = []\n",
    "\n",
    "# Loop through all columns (skip date, serial, model, failure for now)\n",
    "skip_cols = ['date', 'serial_number', 'model', 'failure', 'capacity_bytes']\n",
    "candidate_cols = [c for c in df_seagate.columns if c not in skip_cols]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4acaed31",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Note: In PySpark, doing a loop like this can be slow if not careful.\n",
    "# We will do a single aggregation pass for efficiency.\n",
    "aggregations = []\n",
    "for c in candidate_cols:\n",
    "    # Count how many Nulls or NaNs are in this column\n",
    "    aggregations.append(\n",
    "        count(when(isnan(c) | col(c).isNull(), c)).alias(c)\n",
    "    )\n",
    "\n",
    "# Run the query\n",
    "null_counts = df_seagate.select(aggregations).collect()[0]\n",
    "\n",
    "# Threshold: If more than 30% of data is missing, drop the column.\n",
    "# In hardware, if a sensor works 99% of the time, it's useful. \n",
    "# If it only reports 70% of the time, it's garbage.\n",
    "DROP_THRESHOLD = 0.3 \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d128808",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n--- Feature Selection Report ---\")\n",
    "for c in candidate_cols:\n",
    "    null_count = null_counts[c]\n",
    "    null_pct = null_count / total_rows\n",
    "    \n",
    "    if null_pct < DROP_THRESHOLD:\n",
    "        print(f\"[KEEP] {c}: {null_pct:.2%} missing\")\n",
    "        valid_features.append(c)\n",
    "    else:\n",
    "        # We don't print these to keep output clean, but they are dropped.\n",
    "        pass\n",
    "\n",
    "print(f\"\\nSelected {len(valid_features)} features based on data density.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5bfed159",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4. FEATURE SELECTION STAGE 2: VARIANCE CHECK\n",
    "print(\"\\n--- STAGE 2: VARIANCE CHECK ---\")\n",
    "# If Standard Deviation is 0, the data never changes. It's useless.\n",
    "# We cast to DoubleType to ensure precision during calculation.\n",
    "\n",
    "std_expr = [stddev(col(c).cast(DoubleType())).alias(c) for c in aggregations]\n",
    "\n",
    "# This is a heavy computation, it scans the whole dataset again.\n",
    "print(\"Calculating Standard Deviation for surviving features...\")\n",
    "std_devs = df_seagate.select(std_expr).collect()[0]\n",
    "\n",
    "final_features = []\n",
    "for c in aggregations:\n",
    "    sd = std_devs[c]\n",
    "    # Keep if sd is not None and sd > 0 (has some variance)\n",
    "    # We use a tiny threshold (0.0001) to avoid floating point weirdness\n",
    "    if sd is not None and sd > 0.0001:\n",
    "        final_features.append(c)\n",
    "        # Optional: Print to see what we are keeping\n",
    "        # print(f\"[KEEP] {c} (StdDev: {sd:.2f})\")\n",
    "    else:\n",
    "        print(f\"[DROP] {c} (Zero Variance)\")\n",
    "\n",
    "print(\"-\" * 30)\n",
    "print(f\"Original SMART Features: {len(candidate_cols)}\")\n",
    "print(f\"Post-Null Check:         {len(aggregations)}\")\n",
    "print(f\"Final Selected Features: {len(final_features)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ada0fb14",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4. Final Schema Construction\n",
    "base_cols = ['date', 'serial_number', 'model', 'failure', 'capacity_bytes']\n",
    "final_cols = base_cols + final_features\n",
    "\n",
    "df_final = df_seagate.select(final_cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d92acc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5. Write to Parquet\n",
    "print(\"Writing cleaned dataset to Parquet...\")\n",
    "df_final.write.mode(\"overwrite\").parquet(OUTPUT_PATH)\n",
    "print(\"Done.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
