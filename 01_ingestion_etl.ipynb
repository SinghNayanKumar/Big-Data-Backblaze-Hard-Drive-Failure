{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5640aebc",
   "metadata": {},
   "source": [
    "## Data Ingestion & Optimization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5bb93e52",
   "metadata": {},
   "source": [
    "**Objective:** Raw CSVs are slow and heavy. We need to load them using PySpark (because Pandas will crash with Memory Errors) and convert them to Parquet."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "04221472",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col, to_date, count, when, isnan, stddev\n",
    "from pyspark.sql.types import DoubleType"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2d06d352",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Spark Session Created\n"
     ]
    }
   ],
   "source": [
    "# --- INDUSTRY TECHNIQUE: CONFIGURING SPARK ---\n",
    "# We configure the driver memory to ensure we don't OOM (Out of Memory) locally.\n",
    "# In a cluster (Databricks/AWS EMR), this is handled by the cluster manager.\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"Backblaze_Failure_Prediction\") \\\n",
    "    .config(\"spark.driver.memory\", \"8g\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "print(\"Spark Session Created\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9424e5e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- PATHS ---\n",
    "# Update this to where you unzipped the data\n",
    "RAW_DATA_PATH = \"./data/raw/data_Q3_2025/*.csv\" \n",
    "OUTPUT_PATH = \"./data/parquet_Q3_2025\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "630917c7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading CSVs... this might take a minute.\n"
     ]
    }
   ],
   "source": [
    "# --- STEP 1: READ RAW DATA ---\n",
    "# We use *.csv to tell Spark to read ALL files in that directory as one DataFrame.\n",
    "# inferSchema=True is convenient but slow. In production, we define schemas manually.\n",
    "print(\"Reading CSVs... this might take a minute.\")\n",
    "\n",
    "# samplingRatio=0.01 means \"only read 1% of the data to guess the column types\"\n",
    "df = spark.read.csv(RAW_DATA_PATH, header=True, inferSchema=True, samplingRatio=0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0019e051",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading CSVs (Lazy Mode)...\n"
     ]
    }
   ],
   "source": [
    "print(\"Reading CSVs (Lazy Mode)...\")\n",
    "\n",
    "# inferSchema=False means \"Don't touch the data yet. Just read headers.\"\n",
    "# This should finish in < 2 seconds.\n",
    "df = spark.read.csv(RAW_DATA_PATH, header=True, inferSchema=False)\n",
    "\n",
    "print(\"Initial Read Done.\")\n",
    "\n",
    "# SINCE WE DISABLED INFER_SCHEMA, EVERYTHING IS A STRING.\n",
    "# WE MUST CAST COLUMNS MANUALLY BEFORE DOING MATH.\n",
    "from pyspark.sql.types import DoubleType\n",
    "\n",
    "# We need to update our selection logic to cast strings to numbers\n",
    "# on the fly.\n",
    "# ---------------------------------------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fb8b136",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- UPDATE STEPS 3 & 4 TO HANDLE \"STRING\" INPUTS ---\n",
    "\n",
    "print(\"Filtering for Seagate (ST) drives...\")\n",
    "df_seagate = df.filter(col(\"model\").contains(\"ST\"))\n",
    "df_seagate = df_seagate.withColumn(\"date\", to_date(col(\"date\"), \"yyyy-MM-dd\"))\n",
    "\n",
    "# Cache this. It tells Spark: \"Load this filtered data into that 8GB RAM we promised.\"\n",
    "# This triggers the actual read from disk.\n",
    "df_seagate.cache() \n",
    "\n",
    "print(f\"Counting Rows... (This triggers the actual read)\")\n",
    "total_rows = df_seagate.count()\n",
    "print(f\"Total Seagate Rows: {total_rows}\")\n",
    "\n",
    "# --- STAGE 1: NULL CHECK ---\n",
    "print(\"\\n--- STAGE 1: NULL CHECK ---\")\n",
    "smart_cols = [c for c in df_seagate.columns if c.startswith('smart_') and c.endswith('_raw')]\n",
    "\n",
    "# We cast to Double (float) inside the check to handle the String types\n",
    "select_expr = [count(when(col(c).cast(DoubleType()).isNull(), c)).alias(c) for c in smart_cols]\n",
    "null_counts = df_seagate.select(select_expr).collect()[0]\n",
    "\n",
    "valid_features = []\n",
    "DROP_THRESHOLD = 0.10\n",
    "\n",
    "for c in smart_cols:\n",
    "    missing_count = null_counts[c]\n",
    "    missing_pct = missing_count / total_rows\n",
    "    if missing_pct < DROP_THRESHOLD:\n",
    "        valid_features.append(c)\n",
    "\n",
    "print(f\"Features passed Null Check: {len(valid_features)}\")\n",
    "\n",
    "# --- STAGE 2: VARIANCE CHECK ---\n",
    "print(\"\\n--- STAGE 2: VARIANCE CHECK ---\")\n",
    "\n",
    "# Cast to DoubleType explicitly so StdDev works\n",
    "std_expr = [stddev(col(c).cast(DoubleType())).alias(c) for c in valid_features]\n",
    "\n",
    "std_devs = df_seagate.select(std_expr).collect()[0]\n",
    "\n",
    "final_features = []\n",
    "for c in valid_features:\n",
    "    sd = std_devs[c]\n",
    "    if sd is not None and sd > 0.0001:\n",
    "        final_features.append(c)\n",
    "    else:\n",
    "        # print(f\"[DROP] {c} (Zero Variance)\")\n",
    "        pass\n",
    "\n",
    "print(f\"Final Selected Features: {len(final_features)}\")\n",
    "\n",
    "# --- SAVE ---\n",
    "# When saving, we want to cast these columns to numbers permanently\n",
    "# so our next notebook doesn't have to deal with strings.\n",
    "from pyspark.sql.functions import col\n",
    "\n",
    "cols_to_save = []\n",
    "# Add base cols\n",
    "for c in ['date', 'serial_number', 'model', 'failure', 'capacity_bytes']:\n",
    "    cols_to_save.append(col(c))\n",
    "\n",
    "# Add feature cols (casted to Double)\n",
    "for c in final_features:\n",
    "    cols_to_save.append(col(c).cast(DoubleType()).alias(c))\n",
    "\n",
    "print(\"Saving to Parquet...\")\n",
    "df_seagate.select(cols_to_save).write.mode(\"overwrite\").parquet(OUTPUT_PATH)\n",
    "print(\"Done.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d5f4f0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- STEP 2: INITIAL INSPECTION ---\n",
    "print(f\"Total Rows: {df.count()}\")\n",
    "print(\"Schema:\")\n",
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5954573",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's check the Class Imbalance immediately\n",
    "print(\"Distribution of Failures (0 = Healthy, 1 = Failed):\")\n",
    "df.groupBy(\"failure\").count().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1d27bca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- STEP 3: DATA CLEANING (LIGHT) ---\n",
    "# 1. Cast date column to actual date type\n",
    "df = df.withColumn(\"date\", to_date(col(\"date\"), \"yyyy-MM-dd\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17c83574",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter specifically for Seagate drives (Industry Standard: Don't mix manufacturers in one model initially)\n",
    "# Let's look at the most popular model in the dataset first to decide.\n",
    "print(\"Top 5 Hard Drive Models:\")\n",
    "df.groupBy(\"model\").count().orderBy(col(\"count\").desc()).show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f69474e",
   "metadata": {},
   "source": [
    "1. Filter for our Manufacturer (Seagate).\n",
    "2. Calculate the percentage of missing values for every column.\n",
    "3. Drop columns that are >90% Empty.\n",
    "4. Drop columns that have zero variance (the value is always \"0\" or always \"100\")."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79b5d3c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Filter for Seagate (ST) drives immediately\n",
    "# We do this because different manufacturers use different columns.\n",
    "# Analyzing \"Nulls\" across mixed manufacturers is misleading.\n",
    "\n",
    "# (Assuming 'ST4000DM000' or similar Seagate is top. Let's filter for just Seagate to reduce noise)\n",
    "# Note: You might need to adjust the model string based on the print output above.\n",
    "\n",
    "df_seagate = df.filter(col(\"model\").contains(\"ST\"))\n",
    "\n",
    "print(f\"Seagate Drives Count: {df_seagate.count()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5089c41",
   "metadata": {},
   "outputs": [],
   "source": [
    "#INDUSTRY TECHNIQUE: AUTOMATED NULL ANALYSIS\n",
    "# We want to find which columns are actually populated.\n",
    "print(\"Calculating Null percentages per column... (This takes time)\")\n",
    "\n",
    "# Get total row count for calculation\n",
    "total_rows = df_seagate.count()\n",
    "\n",
    "# Create a list to store valid columns\n",
    "valid_features = []\n",
    "\n",
    "# Loop through all columns (skip date, serial, model, failure for now)\n",
    "skip_cols = ['date', 'serial_number', 'model', 'failure', 'capacity_bytes']\n",
    "candidate_cols = [c for c in df_seagate.columns if c not in skip_cols]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4acaed31",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Note: In PySpark, doing a loop like this can be slow if not careful.\n",
    "# We will do a single aggregation pass for efficiency.\n",
    "aggregations = []\n",
    "for c in candidate_cols:\n",
    "    # Count how many Nulls or NaNs are in this column\n",
    "    aggregations.append(\n",
    "        count(when(isnan(c) | col(c).isNull(), c)).alias(c)\n",
    "    )\n",
    "\n",
    "# Run the query\n",
    "null_counts = df_seagate.select(aggregations).collect()[0]\n",
    "\n",
    "# Threshold: If more than 30% of data is missing, drop the column.\n",
    "# In hardware, if a sensor works 99% of the time, it's useful. \n",
    "# If it only reports 70% of the time, it's garbage.\n",
    "DROP_THRESHOLD = 0.3 \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d128808",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n--- Feature Selection Report ---\")\n",
    "for c in candidate_cols:\n",
    "    null_count = null_counts[c]\n",
    "    null_pct = null_count / total_rows\n",
    "    \n",
    "    if null_pct < DROP_THRESHOLD:\n",
    "        print(f\"[KEEP] {c}: {null_pct:.2%} missing\")\n",
    "        valid_features.append(c)\n",
    "    else:\n",
    "        # We don't print these to keep output clean, but they are dropped.\n",
    "        pass\n",
    "\n",
    "print(f\"\\nSelected {len(valid_features)} features based on data density.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5bfed159",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4. FEATURE SELECTION STAGE 2: VARIANCE CHECK\n",
    "print(\"\\n--- STAGE 2: VARIANCE CHECK ---\")\n",
    "# If Standard Deviation is 0, the data never changes. It's useless.\n",
    "\n",
    "# 1. Create the query to calculate StdDev for survivor columns only\n",
    "# We cast to Double to avoid errors with integer-only columns\n",
    "std_expr = [stddev(col(c).cast(DoubleType())).alias(c) for c in valid_features]\n",
    "\n",
    "\n",
    "# 2. Run the calculation (this scans the data)\n",
    "std_devs = df_seagate.select(std_expr).collect()[0]\n",
    "\n",
    "# 3. Filter\n",
    "final_features = []\n",
    "\n",
    "for c in valid_features:\n",
    "    sd = std_devs[c]\n",
    "    \n",
    "    # Logic: Keep if StdDev > 0.0001 (meaning the data changes)\n",
    "    if sd is not None and sd > 0.0001:\n",
    "        final_features.append(c)\n",
    "    else:\n",
    "        # Optional: Print what we drop\n",
    "        print(f\"[DROP] {c} (Zero Variance)\")\n",
    "\n",
    "print(\"-\" * 30)\n",
    "print(f\"Original Candidate Features: {len(candidate_cols)}\")\n",
    "print(f\"Features after Null Check:   {len(valid_features)}\")\n",
    "print(f\"Final Selected Features:     {len(final_features)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ada0fb14",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- STEP 5: SAVE FINAL DATASET ---\n",
    "\n",
    "# Construct final list of columns to keep\n",
    "# We keep the identifiers + the features that passed both checks\n",
    "final_cols = ['date', 'serial_number', 'model', 'failure', 'capacity_bytes'] + final_features\n",
    "\n",
    "print(f\"Selecting {len(final_cols)} columns...\")\n",
    "df_final = df_seagate.select(final_cols)\n",
    "\n",
    "print(\"Writing cleaned dataset to Parquet...\")\n",
    "df_final.write.mode(\"overwrite\").parquet(OUTPUT_PATH)\n",
    "print(\"Done. ETL Pipeline Complete.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
